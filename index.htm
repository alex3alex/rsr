<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="favicon.ico">

    <title>Human and Machine Judgements about Russian Semantic Relatedness
</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="sticky-footer.css" rel="stylesheet">

  </head>

  <body>

    <!-- Begin page content -->
    <div class="container">
      <div class="page-header">
        <h1>Human and Machine Judgements about Russian Semantic Relatedness
</h1>
      </div>

      <p> This page contains several open language resources for semantic relatednes in Russian language. We present five semantic relatedness resources for Russian, each being a list of triples (word_i, word_j, similarity_ij). Four of them are designed for evaluation of semantic relatedness, each complementing another in terms of relation type. These benchmarks were used in a shared task on Russian semantic similarity. One of the best systems was used to generate the fifth resource – an open distributional thesaurus of Russian. Multiple evaluations of this thesaurus indicate its state-of-the-art quality.  
</p>

     
<br />
<p class="lead">HJ: Human Judgements of Word Pairs</p>
      

<p>HJ dataset contains human judgements on 398 word pairs that were translated to Russian from the widely used benchmarks for English: MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965) and WordSim353 (Filkenstein et al., 2001). In order to collect human judgements,  an in-house crowdsourcing system was used. Volunteers on Facebook and Twitter were invited to participate in the experiment. Each annotator has been provided with a random assignment consisting of 15 word pairs selected from the 398 preliminarily prepared pairs, and has been asked to assess the similarity of each pair. The possible options were “not similar at all”, “weak similarity”, “moderate similarity”, and “high similarity”. Ordinal Krippendorff’s alpha of 0.49 indicates a moderate agreement of annotators in this experiment. To evaluate a similarity measure using this dataset one should calculate Spearman’s rank correlation coefficient ρ between a vector of human judgments and the scores of a given system. 
</p>

<p><a href="https://github.com/nlpub/russe-evaluation/blob/master/russe/evaluation/hj.csv">Download HJ dataset</a> in the CSV format "word-i word-j similarity-ij"</p>


<br />
 <p class="lead">RT: Synonyms and Hypernyms from the Thesaurus RuThes</p>

<p> This dataset follows structure of the BLESS dataset (Baroni and Lenci, 2011). Each target word has the same number of related and unrelated source words. The dataset contains 114,066 relations for 6,832 nouns. Half of these relations are synonyms and hypernyms from the RuThes Lite thesaurus (Loukachevitch et al., 2014) and half of them are unrelated words. To generated negative pairs we used simple procedure described in (Panchenko et al., 2015). We filtered false negative relations for 1,008 source words with the help of human annotators. Each negative relation in this subset was annotated by at least two annotators. To evaluate a similarity measure using this dataset one needs to calculate  average precision (Panchenko et al., 2015).</p>

<p><a href="https://github.com/nlpub/russe-evaluation/blob/master/russe/evaluation/rt-test.csv">Download RT dataset (test)</a> in the CSV format "word-i word-j similarity-ij"</p>
<p><a href="https://raw.githubusercontent.com/nlpub/russe-evaluation/master/russe/evaluation/rt.csv">Download RT dataset (full)</a> in the CSV format "word-i word-j similarity-ij"</p>


<br />
<p class="lead">AE: Cognitive Associations from the Sociation.org Experiment
</p>

<p> The structure of this dataset is the same as the structure of the RT dataset: each source word has the same number of related and unrelated target words. Related word pairs were sampled from a Russian Web associative experiment. In the experiment, users were asked to provide a reaction to an input stimulus source word, e.g.: man → woman, time → money, and so on. The strength of association in this experiment is quantified by the number of respondents providing the same stimulus-reaction pair. Associative thesauri typically contain a mix of synonyms, hyponyms, meronyms and other types, making relations asymmetric. To build the dataset we have selected target words with the highest association with the stimulus in Sociation.org data. Like with the other datasets, we used only single-word nouns. Similarly to the RT dataset, we automatically generated negative word pairs and manually filtered false negatives for a subset of 1,501 pairs.  As with the RT dataset,  to evaluate a similarity measure based on this dataset one should use average precision.</p>

<p><a href="https://github.com/nlpub/russe-evaluation/blob/master/russe/evaluation/ae2-test.csv">Download AE dataset (test)</a> in the CSV format "word-i word-j similarity-ij"</p>
<p><a href="https://raw.githubusercontent.com/nlpub/russe-evaluation/master/russe/evaluation/ae2.csv">Download AE dataset (full)</a> in the CSV format "word-i word-j similarity-ij"</p>


<br />
<p class="lead">MJ: Machine Judgements of Word Pairs from the RUSSE Shared Task</p>

<p> This dataset contains 12,886 word pairs coming from HJ, RT, and AE datasets. In contrast to original hand-labeled binary labels (see Table 1) these pairs have continuous relatedness scores. To estimate these scores we averaged 105 submissions of the shared task on Russian semantic similarity, RUSSE (Panchenko et al., 2015). Each run consisted of 12,886 word pairs along with similarity scores.</p>

<p><a href="https://github.com/nlpub/russe-evaluation/blob/master/russe/evaluation/mj.csv">Download MJ dataset</a> in the CSV format "word-i word-j similarity-ij"</p>

<br />
 <p class="lead">DT: Open Russian Distributional Thesaurus</p>
      
<p>While resources presented above are accurate and represent different kind of semantics their low coverage let us use them only for evaluation and training. In this section we present a large scale resource in the same  (wi,wj,sij) format -- the first open Russian distributional thesaurus. </p>

     
<p> In order to build the distributional thesaurus, we used the skip-gram model (Mikolov et al., 2013) trained on a 12.9 billion word collection of books in Russian. According to the results of our participation in the shared task on Russian semantic similarity (Panchenko et al., 2015), this approach scored in the top 5 among 105 submissions (Arefyev et al., 2015). At the same time, the approach is completely unsupervised and language independent as we do not use any preprocessing except tokenization. More specifically, to build a thesaurus we trained the model on Russian books extracted from the digital library lib.rus.ec. Following our prior experiments (Arefyev et al., 2015) we have selected the following parameters for the model: minimal word frequency -- 5, number of dimensions in a word vector -- 500, three or five iterations of the learning algorithm over the input corpus, context window size of 1, 2, 3, 5, 7 and 10 words.  For the most frequent 932,000 words, we calculated 250 nearest neighbours with the cosine similarity between word vectors. These related words were lemmatized using PyMorphy2. An important added value of our work is engineering. Training of a model takes up to five days on a r3.8xlarge Amazon EC2 instance featuring 32 cores and 244 GB of RAM. Furthermore, calculation of the neighbours takes up to ten days for only one. Not to mention the time needed to test different configurations of the model. On the other hand, the resulting DT is a CSV file that can be simply used from any environment.  </p>   

<p>
      Parameters of the model used to generate this distributional thesaurus:
      <ul>
      <li>Model: skip-gram</li>
      <li>Corpus: 150Gb sample of lib.rus.ec</li>
      <li>Number of thesaurus entries (words): 932,000</li>
      <li>Context window size: 10 words</li>
      <li>Number of dimensions: 500</li>
      <li>Number of iterations: 3</li>
      <li>Minimal word frequency: 5</li>
      </ul>
</p>


 <p>
      <a href="https://s3-eu-west-1.amazonaws.com/dsl-research/distrib_thes/3attempt/all.norm-sz500-w10-cb0-it3-min5.w2v.vocab_1100000_similar250.gz">Download DT dataset</a> in the CSV format "word-i word-j similarity-ij" (1.8 Gb)

      <br />

      <a href="https://s3-eu-west-1.amazonaws.com/dsl-research/distrib_thes/3attempt/all.norm-sz500-w10-cb0-it3-min5.w2v.vocab_1100000_similar250.gz">Download word vectors</a> in the format of <a href="https://code.google.com/p/word2vec/">word2vec</a> (14 Gb)

      <br />

      <a href="https://github.com/nlpub/russe-evaluation/tree/master/russe/measures/word2vec">How to load word vectors</a>, Python examples on Github.

      <br />
  </p>


<br />
 <p class="lead">Evaluation Scripts</p>
      
<p>All evaluation scripts that can be used to measure performance of similarity measures with the listed above datasets are available at <a href="https://github.com/nlpub/russe-evaluation/tree/master/russe/evaluation">Github</a>.</p>




    </div>




    <footer class="footer">
      <div class="container">
        <p class="text-muted">For further information contact Alexander Panchenko  (surname dot name at g mail).</p>
      </div>
    </footer>


    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
